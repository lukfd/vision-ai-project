{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fb8eab0",
   "metadata": {},
   "source": [
    "# Basic CNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43fa8e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: I didn't have much time so threw this together in pytorch, we can refactor if necessary\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "from pyarrow import parquet\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "photo_directory = \"data/clean/reduced_photos\"\n",
    "\n",
    "device = (\n",
    "    torch.accelerator.current_accelerator().type\n",
    "    if torch.accelerator.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1955451",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# NOTE: could be moved to clean script\n",
    "b = parquet.read_table(\"data/clean/reduced_business_details.parquet\")\n",
    "p = parquet.read_table(\"data/clean/reduced_photo_details.parquet\")\n",
    "photos_scores = p.join(b, \"business_id\").combine_chunks()\n",
    "\n",
    "photos_scores = photos_scores.add_column(\n",
    "    0,\n",
    "    \"star_category\",\n",
    "    pa.array(\n",
    "        photos_scores.select([\"stars\"])\n",
    "        .to_pandas()\n",
    "        .stars.apply(\n",
    "            lambda r:  (0. if r <= 3 else 1.) if r <= 4 else 2.)\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cfbd31",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# TODO: image normalization - I think we need a real transform at some point?\n",
    "class ImageData(Dataset):\n",
    "    def __init__(\n",
    "        self, labels, filenames, width=256, height=256, transform=None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.transform = transform\n",
    "        self.labels = labels\n",
    "        self.filenames = filenames\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(\n",
    "            f\"{photo_directory}/{self.filenames[index]}.jpg\"\n",
    "        )  # use pillow to open a file\n",
    "        img = img.resize((self.width, self.height))  # resize the file to 256x256\n",
    "        img = img.convert(\"RGB\")  # convert image to RGB channel\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        img = np.asarray(\n",
    "            img\n",
    "        ).transpose(\n",
    "            -1, 0, 1\n",
    "        )  # we have to change the dimensions from width x height x channel (WHC) to channel x width x height (CWH)\n",
    "        img = img / 255\n",
    "        img = (\n",
    "            torch.from_numpy(np.asarray(img)).to(torch.float32).to(device)\n",
    "        )  # create the image tensor\n",
    "        label = (\n",
    "            nn.functional.one_hot(\n",
    "                torch.from_numpy(np.asarray(self.labels[index]))\n",
    "                .to(torch.long)\n",
    "                .to(device)\n",
    "                , num_classes=3\n",
    "            )\n",
    "        )\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "yelp_photos = ImageData(\n",
    "    photos_scores.select([\"star_category\"]).to_pandas().star_category,\n",
    "    photos_scores.select([\"photo_id\"]).to_pandas().photo_id,\n",
    ")\n",
    "# NOTE: change batch size for actual leanring\n",
    "\n",
    "train, val, test = torch.utils.data.random_split(yelp_photos, [0.5, 0.3, 0.2])\n",
    "training_loader = DataLoader(train, batch_size=30, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=len(test))\n",
    "validation_loader = DataLoader(val, batch_size=len(val))\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.convolutional = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 132, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(19008, 1500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1500, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 3),\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(torch.flatten(self.convolutional(x), 1))\n",
    "\n",
    "\n",
    "epoch_number = 1\n",
    "best_vloss = 10000\n",
    "basic_cnn = CNN().to(device)\n",
    "opt = torch.optim.Adam(basic_cnn.parameters(), lr = .001)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "print(summary(basic_cnn))\n",
    "basic_cnn(yelp_photos.__getitem__(1)[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: export model WITHIN LOOP\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('logs/basic_cnn_{}'.format(timestamp))\n",
    "\n",
    "def train_one_epoch(epoch_index, m: nn.Module, loader, o, lf, writer=writer):\n",
    "    running_loss = 0.\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(loader):\n",
    "        inputs, labels = inputs.to(device).to(torch.float32), labels.to(device).to(torch.float32).squeeze()\n",
    "        \n",
    "        o.zero_grad()\n",
    "        outputs = m(inputs)\n",
    "        loss = lf(outputs, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "\n",
    "    return m, running_loss / i\n",
    "\n",
    "for epoch in range(31):\n",
    "    print(f\"EPOCH {epoch_number}\")\n",
    "    basic_cnn.train(True)\n",
    "    basic_cnn, tloss = train_one_epoch(epoch, basic_cnn, training_loader, opt, loss_fn, writer)\n",
    "    \n",
    "    basic_cnn.eval()\n",
    "\n",
    "    # TODO: add val loss check\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = next(iter(validation_loader))\n",
    "        inputs, labels = inputs.to(device).to(torch.float32), labels.to(device).to(torch.float32).squeeze()\n",
    "\n",
    "        outputs = basic_cnn(inputs)\n",
    "        vloss = loss_fn(outputs, labels).item()\n",
    "\n",
    "    print(f\"LOSS train {round(tloss, 5)}, validation {round(vloss, 5)}\")\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : tloss, 'Validation' : vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "    if vloss < best_vloss:\n",
    "        best_vloss = vloss\n",
    "        model_path = f\"models/basic_cnn/{epoch_number}_{timestamp}_{round(vloss, 3)}\"\n",
    "        torch.save(basic_cnn.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57676533",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "# TODO: sklearn for scoring\n",
    "saved_models = os.listdir(\"models/basic_cnn\")\n",
    "best_model = [it for it in saved_models \n",
    "              if str(min([float(i.split(\"_\")[-1]) for \n",
    "              i in os.listdir(\"models/basic_cnn\")])) in it][0]\n",
    "\n",
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    outputs = basic_cnn(inputs)\n",
    "    print(str(classification_report(labels.argmax(dim=1).cpu().detach().numpy(), \n",
    "                                    outputs.argmax(dim=1).cpu().detach().numpy())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision-ai-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
